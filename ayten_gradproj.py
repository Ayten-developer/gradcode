# -*- coding: utf-8 -*-
"""ayten's_gradproj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aymqY5zlxD2R8EXArZvdDXv8PyLJHM10
"""

# pip install pdfplumber

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
 # data processing, CSV file I/O (e.g. pd.read_csv)

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Functions Definitons:

**ReadPDF to Text**
"""

import pdfplumber
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords

stop = stopwords.words('english')
def pdf2text(file_path):
    '''Iterate over pages and extract text'''
    text = ''
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

"""# **Cleaning csv**"""

def preprocess_text(text, name):
    # lowercasing
    lowercased_text = text.lower()

    # cleaning
    import re
    remove_punctuation = re.sub(r'[^\w\s]', '', lowercased_text)
    remove_white_space = remove_punctuation.strip()

    # Tokenization = Breaking down each sentence into an array
    from nltk.tokenize import word_tokenize
    tokenized_text = word_tokenize(remove_white_space)

    # Stop Words/filtering = Removing irrelevant words
    from nltk.corpus import stopwords
    stopwords = set(stopwords.words('english'))
    stopwords_removed = [word for word in tokenized_text if word not in stopwords]

    # Stemming = Transforming words into their base form
    from nltk.stem import PorterStemmer
    ps = PorterStemmer()
    stemmed_text = [ps.stem(word) for word in stopwords_removed]

    # Putting all the results into a dataframe
    data_frame = pd.DataFrame({
        'NAME': [name],
        'DOCUMENT': [text],
        #'LOWERCASE': [lowercased_text],
        #'CLEANING': [remove_white_space],
        #'TOKENIZATION': [tokenized_text],
        ###'STOP-WORDS': [stopwords_removed],
        'PREPROCESSED': [' '.join(stemmed_text)]  # Store as string
    })

    return data_frame#returns single row ["name","document","stemming"]

def preprocessing(corpus):
    # Create an empty DataFrame
    data_frame = pd.DataFrame(columns=['NAME', 'DOCUMENT',  'PREPROCESSED'])#LOWERCASE', 'CLEANING', 'TOKENIZATION', 'STOP-WORDS',

    # Running preprocessing one by one
    for index, row in corpus.iterrows():
        preprocessed_row = preprocess_text( row['description'],row['name'])
        data_frame = pd.concat([data_frame,preprocessed_row], ignore_index=True)

    return data_frame

"""# **TF-IDF Calculation whole**"""

def calculate_tfidf(corpus):#caculate TF-IDF for dataframe
    # global vectorizer#acess  the globally defined vectorizer

    # Convert stemming words to string for vectorization
    pre_processed_docs = corpus['PREPROCESSED']
    #corpus=corpus.apply(' '.join)
    vectorizer = TfidfVectorizer()

    tfidf_matrix = vectorizer.fit_transform(pre_processed_docs)

    # Retrieve feature names (unique words) learned by tf-idf vectorizer
    feature_names = vectorizer.get_feature_names_out()

    # Combine feature names above and weights -> array -> data frame
    df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

    # Separate the name column
    df_tfidf['NAME'] = corpus['NAME'].values
    df_tfidf['DOCUMENT'] = corpus['DOCUMENT'].values
    df_tfidf['PREPROCESSED'] = corpus['PREPROCESSED'].values



    return df_tfidf

"""# **COSINE SIMILARITY whole**"""

import pandas as pd
# import seaborn as sns
df = pd.read_csv('/Users/Ayten/Desktop/gradcode/items_en.csv')
print(df.isnull().sum())

vectorizer = TfidfVectorizer()#Globally defined vectorizer

print("Before dropping entries of null values")
df.info() #If numerical values ,such as of age of type object , search for values enterd miscorrectly of such & replace them w/ Mean or mode or median

#replace null values w/ mode in Difficulty
mode_difficulty = df['Difficulty'].mode()[0]
# print("mode difficlty: ",mode_difficulty)
df.fillna({'Difficulty': mode_difficulty}, inplace=True)
#replace null values w/ mean in nb_views
mean_views = df['nb_views'].mean()
df.fillna({'nb_views': mean_views}, inplace=True)
# print(df.isnull().sum())
min_view = df['nb_views'].min()
print("min num of views:",min_view)
error_views = df[df['nb_views'] < 1]['nb_views']
print("values less than 1:", error_views)#none

#dropping description
# df.drop(columns=['description'], inplace=True)

df.dropna(subset=['description'], inplace=True)#drop all rows with null descriptions

"""**Data visualization**"""

df['language'].value_counts() #drop this colomn insignificant data not distributed

# df['Job'].value_counts()
# df['Job'].value_counts()
print(df['Job'].value_counts())
df.drop(columns=['Job'], inplace=True)
# df.head()
# df.iloc[0]['Job']

df['type'].value_counts()
print("duplicated rows:",df.duplicated().sum())

#dropping irrelevant colomns
df = df.drop(columns=['language','created_at','Software','Theme','duration','item_id'])


df.head()

"""**Term Frequency: TF of a term or word is the number of times the term appears in a document compared to the total number of words in the document. Inverse Document Frequency: IDF of a term reflects the proportion of documents in the corpus that contain the term.**

# **NLP**
"""

# from sklearn.feature_extraction.text import TfidfVectorizer
# # to split each string in description column
# df['description'] = df['description'].astype(str)
# df['description'] = df['description'].apply(lambda x: x.split())
# # df.iloc[0]['description']

"""**preprocessing of dataset description**"""

'''from sklearn.feature_extraction.text import TfidfVectorizer

tdif = TfidfVectorizer(stop_words='english')
#converts colomn values to string
df['description'] = df['description'].astype(str)
docs_desc=df['description'].astype(str)
#TF-IDF matrix -> rows: doc1,doc2,... ; colomns: unique word in docs.value score
# Values in matrix represents TF-IDF scores in each document
tdif_matrix = tdif.fit_transform(docs_desc)#converts text data into TF-IDF matrix
# print(type(tdif_matrix))
print(tdif_matrix)
'''
dataset=preprocessing(df)
df

dataset.head()

#print(df.loc[0, 'description'])
tf_idf=calculate_tfidf(dataset)
tf_idf.head()

non_zero_counts = tf_idf.astype(bool).sum(axis=0)
non_zero_counts#gets values where TF-IDF are not zeros

# vectorizer.get_feature_names_out()

"""# Convert TF-IDF Matrix to DataFrame

def get_recommendation(name, input_text, tf_idf_df, k=5):
    global vectorizer  # Access the global vectorizer

    # Preprocess the input text
    preprocessed_input = preprocess_text(input_text, name)['PREPROCESSED'][0]

    # Check if input_text is already in tf_idf_df['DOCUMENT']
    if input_text not in tf_idf_df['DOCUMENT'].values:
        # If not found, append it to tf_idf_df
        new_row = pd.DataFrame({
            'NAME': [name],
            'DOCUMENT': [input_text],
            'PREPROCESSED': [preprocessed_input]
        })
        tf_idf_df = pd.concat([tf_idf_df, new_row], ignore_index=True)

        # Update tfidf_vectorizer with new documents
        pre_processed_docs = tf_idf_df['PREPROCESSED']
        tfidf_matrix = vectorizer.fit_transform(pre_processed_docs)

        # Vectorize the input text using existing tfidf_vectorizer
        input_tfidf = vectorizer.transform([preprocessed_input])
    else:
        # Vectorize the input text using existing tfidf_vectorizer
        input_tfidf = vectorizer.transform([preprocessed_input])

        # Calculate cosine similarities with existing documents
        cosine_similarities = linear_kernel(input_tfidf, tfidf_matrix)

        # Get document indices based on similarity scores
        sim_scores = list(enumerate(cosine_similarities[0]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[:k]
        document_indices = [i[0] for i in sim_scores]

        # Return top k document names
        top_documents = tf_idf_df.loc[document_indices, 'NAME'].values

        return top_documents
    #returns the tfid dataframe with name appended as last col,trained vectorizer
input_text="Once you sync your one drive library to your computer you can work with these files just as you did, with other files in your file system. You can find your sink library in the Navigation Pane from your windows Explorer.Your library will appear in the one drive folder an other libraries from SharePoint sites will appear here.So if I'm in word for example, I can open the files in this folder or I can create a new file and save it to my sync folder.When I'm online. The new file immediately gets uploaded into the site library. Same if you delete a file on your sync folder. The file will be deleted in share point at the next sync.What happened if you don't have permissions to do it?The file will get re download it into the library folder during the next sync.Same if you don't have permissions to add a new file. It just doesn't upload and you see an error mark on the file in your sink folder.A1 drive menu. Let's me start activities with files in the sink folder. Let's have a look.Here, I right click on a document.I can then choose to share the folder to display it in the browser and to access the synchronized folders.I can also open the OneDrive Menu at anytime from my windows taskbar.These options allow me to manage the synchronization."
get_recommendation("text",input_text,tf_idf)
"""

'''# convert matrix into dataframe
tfidf_df = pd.DataFrame(tdif_matrix.toarray(), columns=tdif.get_feature_names_out())
value_iloc = tfidf_df.iloc[0,15364]
print(value_iloc)
'''

#print(tfidf_df)

"""# **GetRecommendation**"""

def get_recommendations(name, tf_idf):
    from sklearn.metrics.pairwise import linear_kernel
    # Compute the cosine similarity matrix: dot product
    cosine_sim = linear_kernel(tf_idf.iloc[:,:-3], tf_idf.iloc[:,:-3])
    #maps between name & index
    indices = pd.Series(tf_idf.index, index=tf_idf['NAME'])
    idx = indices[name] #index of record
    # Get the pairwsie similarity scores of all courses with that course
    sim_scores = list(enumerate(cosine_sim[idx]))
    #sort courses based on similarity score
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # get score of most 10 similar courses
    sim_scores = sim_scores[1:11]
    #creates new list containg
    course_indices = [i[0] for i in sim_scores]
    # Return the top 10 most similar courses
    return df['name'].iloc[course_indices],df['description'].iloc[course_indices]
    


df.head()
# indices = pd.Series(df2.index, index=df2['title'])

get_recommendations('Recycle bin',tf_idf)

#save last 3 csv and download
last_3_columns_df = tf_idf.iloc[:, -3:]
# last_3_columns_df
last_3_columns_df.to_csv('last_3_columns_tfidf.csv', index=False)

# record_name = df.loc[df['name'] == 'Recycle bin']
# record_description = df.loc[df['name'] == 'Recycle bin', 'description'].values
# print("Record Name: Recycle bin")
# print("\n")
# print("Record Description:",record_description)


import nltk
nltk.download('stopwords')
nltk.download('punkt')

file_path = '/Users/Ayten/Desktop/gradcode/example.pdf'
extracted_text = pdf2text(file_path)
# extracted_text = extracted_text.replace('%', 't')
print(extracted_text)

# text = "When you delete a file or a folder, OneDrive gives you a chance to undo your decision but even if you miss the opportunity, you can s;ll get the file back. In the naviga;on pane, click Recycle bin. As you can see OneDrive doesn't really delete items and moves them here so you can get"
# name = "example.pdf"
# CSV_FILE = '/content/last_3_columns_tfidf.csv'
# csv_df = pd.read_csv(CSV_FILE)
# preprocessed_text = preprocess_text(text, name)
# # csv_df = csv_df.append(preprocessed_text, ignore_index=True)
# csv_df = pd.concat([csv_df, preprocessed_text], ignore_index=True)
# csv_df.tail(5)
# # csv_df.to_csv(CSV_FILE, index=False)
# tfidf_result = calculate_tfidf(csv_df)
# recommendations = get_recommendations(name,tfidf_result)
# recommendations
# print(type(recommendations))

# import pandas as pd

# def combine_columns(df, col1, col2):
#     """
#     Combines two columns of a DataFrame into a pandas Series.

#     Parameters:
#     df (pd.DataFrame): The DataFrame containing the columns to combine.
#     col1 (str): The name of the first column.
#     col2 (str): The name of the second column.

#     Returns:
#     pd.Series: A Series where each element is a combination of the values from col1 and col2.
#     """
#     return df[col1].astype(str) + ' ' + df[col2].astype(str)

# # Example usage:
# data = {'x': [1, 2, 3], 'y': ['A', 'B', 'C']}
# df = pd.DataFrame(data)

# combined_series = combine_columns(df, 'x', 'y')
# print(combined_series)